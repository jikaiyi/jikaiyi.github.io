<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><big> Shaofeng Zou </big></div>
<div class="menu-item"><a href="index.html"><big>Home</big></a></div>
<div class="menu-item"><a href="research.html"><big>Publication</big></a></div>
<div class="menu-item"><a href="teaching.html"><big>Teaching	</big></a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=E0A3lSIAAAAJ&hl=en"><big>Google Scholar</big></a></div>
<div class="menu-item"><a href="service.html"><big>Service</big></a></div>
</td>
<td id="layout-content">
<h2>Conference Publications</h2>
<ul>
<li><p>Provably Faster Algorithms for Bilevel Optimization <a href="https://github.com/JunjieYang97/MRVRBO">Code</a> <br /> Junjie Yang, Kaiyi Ji, Yingbin Liang <br /> Submitted to Conference on Neural Information Processing Systems (NeurIPS) 2021. </p>
</li>
<li><p>Bilevel Optimization: Nonasymptotic Analysis and Faster Algorithms <a href="https://github.com/JunjieYang97/stocBiO">Code</a> <br /> Kaiyi Ji, Junjie Yang, Yingbin Liang <br /> International Conference on Machine Learning (ICML) 2021 <br /> Short version appears at ICML 2021 Workshop: Beyond First-Order Methods in ML Systems.</p>
</li>
<li><p>Convergence of Meta-Learning with Task-Specific Adaptation over Partial Parameters <br /> Kaiyi Ji, Jason D. Lee, Yingbin Liang, H. Vincent Poor <br /> Conference on Neural Information Processing Systems (NeurIPS) 2020.</p>
</li>
<li><p>History-Gradient Aided Batch Size Adaptation for Variance Reduced Algorithms <br /> Kaiyi Ji, Zhe Wang, Bowen Weng, Yi Zhou, Wei Zhang, Yingbin Liang, <br /> International Conference on Machine Learning (ICML) 2020 <b>(21.8% acceptance rate)</b>.</p>
</li>
<li><p>Proximal Gradient Algorithm with Momentum and Flexible Parameter Restart for Nonconvex Optimization <br /> Yi Zhou, Zhe Wang, Kaiyi Ji, Yingbin Liang, Vahid Tarokh <br /> International Joint Conferences on Artificial Intelligence Organization (IJCAI) 2020 <b>(12.6% acceptance rate)</b>.</p>
</li>
<li><p>Robust Stochastic Bandit Algorithms under Probabilistic Unbounded Adversarial Attack <br /> Ziwei Guan, Kaiyi Ji,  Donald J. Bucci Jr, Timothy Hu, Joseph Palombo, Michael Liston, Yingbin Liang <br /> AAAI Conference on Artificial Intelligence (AAAI) 2020 <b>(20.6% acceptance rate)</b>.</p>
</li>
<li><p>SpiderBoost and Momentum: Faster Stochastic Variance Reduction Algorithms <br /> Zhe Wang, Kaiyi Ji,  Yi Zhou, Yingbin Liang, Vahid Tarokh <br /> Conference on Neural Information Processing Systems (NeurIPS) 2019 <b>(21% acceptance rate)</b>.</p>
</li>
<li><p>Improved Zeroth-Order Variance Reduced Algorithms and Analysis for Nonconvex Optimization <br /> Kaiyi Ji, Zhe Wang, Yi Zhou, Yingbin Liang <br /> International Conference on Machine Learning (ICML) 2019 <b>(22.5% acceptance rate)</b>.</p>
</li>
<li><p>Minimax Estimation of Neural Net Distance <br /> Kaiyi Ji, Yingbin Liang <br /> Conference on Neural Information Processing Systems (NeurIPS) 2018 <b>(21% acceptance rate)</b>.</p>
</li>
<li><p>Asymptotic Miss Ratio of LRU Caching with Consistent Hashing <br /> Kaiyi Ji, Guocong Quan, Jian Tan <br /> IEEE INFOCOM 2018 <b>(19% acceptance rate)</b>.</p>
</li>
<li><p>LRU Caching with Dependent Competing Requests <br /> Guocong Quan, Kaiyi Ji, Jian Tan <br /> IEEE INFOCOM 2018 <b>(19% acceptance rate)</b>.</p>
</li>
<li><p>On Resource Pooling and Separation for LRU Caching <br />   Jian Tan, Guocong Quan, Kaiyi Ji,  Ness Shroff <br /> ACM SIGMETRICS 2018 <b>(20% acceptance rate)</b>.</p>
</li>
</ul>
<h2>Journal Publications and Preprints</h2>
<ul>
<li><p>Boosting One-Point Derivative-Free Online Optimization via Residual Feedback <br /> Yan Zhang, Yi Zhou, Kaiyi Ji, Michael M. Zavlanos <br /> To be submitted a journal. </p>
</li>
<li><p>Lower Bounds and Accelerated Algorithms for Bilevel Optimization <br /> Kaiyi Ji, Yingbin Liang <br /> Submitted to Journal of Machine Learning Research (JMLR) 2021.</p>
</li>
<li><p>A New One-Point Residual-Feedback Oracle For Black-Box Learning and Control <br />  Yan Zhang, Yi Zhou, Kaiyi Ji, Michael M. Zavlanos <br /> Automatica 2021.</p>
</li>
<li><p>Theoretical Convergence of Multi-Step Model-Agnostic Meta-Learning <br /> Kaiyi Ji, Junjie Yang, Yingbin Liang <br /> Journal of Machine Learning Research (JMLR) 2021 <br /> Short version appears at ICML 2021 Workshop: Beyond First-Order Methods in ML Systems</p>
</li>
<li><p>Understanding Estimation and Generalization Error of Generative Adversarial Networks <br /> Kaiyi Ji, Yi Zhou, Yingbin Liang <br /> <b>IEEE Trans. on Information Theory (TIT), 2021</b>.</p>
</li>
<li><p>When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models? <br /> Tengyu Xu, Yi Zhou, Kaiyi Ji, Yingbin Liang <br /> <b>Stat</b>, Special Issue of Deep Learning from Statistical Perspective, 2021</p>
</li>
<li><p>Learning Latent Features with Pairwise Penalties in Matrix Completion <br />  Kaiyi Ji, Jian Tan, Yuejie Chi, Jinfeng Xu <br />  <b>IEEE Trans. on Signal Processing (TSP), 2020</b> <br /> Short version as an invited paper to a special session at the IEEE SAM 2020</p>
</li>
<li><p>On Resource Pooling and Separation for LRU Caching <br />  Jian Tan, Guocong Quan, Kaiyi Ji,  Ness Shroff <br /> Proceedings of the ACM on Measurement and Analysis of Computing Systems (POMACS), 2018</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
