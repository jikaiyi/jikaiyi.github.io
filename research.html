<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category"><big> Kaiyi Ji </big></div>
<div class="menu-item"><a href="index.html"><big>Home</big></a></div>
<div class="menu-item"><a href="research.html"><big>Publication</big></a></div>
<div class="menu-item"><a href="teaching.html"><big>Teaching	</big></a></div>
<div class="menu-item"><a href="service.html"><big>Experience</big></a></div>
<div class="menu-item"><a href="https://scholar.google.com/citations?user=E0A3lSIAAAAJ&hl=en"><big>Google Scholar</big></a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications</h1>
</div>
<h2>Dissertation</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/2108.00330">Bilevel Optimization for Machine Learning: Algorithm Design and Convergence Analysis</a> <br /> Ph. D. Dissertation, The Ohio State University, August 2021.</p>
</li>
</ul>
<h2>Conference Papers</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/2106.04692.pdf">Provably Faster Algorithms for Bilevel Optimization</a> <a href="https://github.com/JunjieYang97/MRVRBO">&lt;Code&gt;</a> <br /> Junjie Yang, <b>Kaiyi Ji</b>, Yingbin Liang <br /> Conference on Neural Information Processing Systems (NeurIPS) 2021. <b>Spotlight, &lt;3% acceptance rate</b> </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2010.07962">Bilevel Optimization: Nonasymptotic Analysis and Faster Algorithms</a> <a href="https://github.com/JunjieYang97/stocBiO">&lt;Code&gt;</a> <br /> <b>Kaiyi Ji</b>, Junjie Yang, Yingbin Liang <br /> International Conference on Machine Learning (ICML) 2021 <b>(21.4% acceptance rate)</b> <br /> Short version appears at ICML 2021 Workshop: Beyond First-Order Methods in ML Systems.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2006.09486.pdf">Convergence of Meta-Learning with Task-Specific Adaptation over Partial Parameters</a> <br /> <b>Kaiyi Ji</b>, Jason D. Lee, Yingbin Liang, H. Vincent Poor <br /> Conference on Neural Information Processing Systems (NeurIPS) 2020 <b>(20.1% acceptance rate)</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1910.09670.pdf">History-Gradient Aided Batch Size Adaptation for Variance Reduced Algorithms</a> <br /> <b>Kaiyi Ji</b>, Zhe Wang, Bowen Weng, Yi Zhou, Wei Zhang, Yingbin Liang, <br /> International Conference on Machine Learning (ICML) 2020 <b>(21.8% acceptance rate)</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2002.11582.pdf">Proximal Gradient Algorithm with Momentum and Flexible Parameter Restart for Nonconvex Optimization</a> <br /> Yi Zhou, Zhe Wang, <b>Kaiyi Ji</b>, Yingbin Liang, Vahid Tarokh <br /> International Joint Conferences on Artificial Intelligence Organization (IJCAI) 2020 <b>(12.6% acceptance rate)</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2002.07214.pdf">Robust Stochastic Bandit Algorithms under Probabilistic Unbounded Adversarial Attack</a> <br /> Ziwei Guan, <b>Kaiyi Ji</b>,  Donald J. Bucci Jr, Timothy Hu, Joseph Palombo, Michael Liston, Yingbin Liang <br /> AAAI Conference on Artificial Intelligence (AAAI) 2020 <b>(20.6% acceptance rate)</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/file/512c5cad6c37edb98ae91c8a76c3a291-Paper.pdf">SpiderBoost and Momentum: Faster Stochastic Variance Reduction Algorithms</a> <br /> Zhe Wang, <b>Kaiyi Ji</b>,  Yi Zhou, Yingbin Liang, Vahid Tarokh <br /> Conference on Neural Information Processing Systems (NeurIPS) 2019 <b>(21% acceptance rate)</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="http://proceedings.mlr.press/v97/ji19a/ji19a.pdf">Improved Zeroth-Order Variance Reduced Algorithms and Analysis for Nonconvex Optimization</a> <br /> <b>Kaiyi Ji</b>, Zhe Wang, Yi Zhou, Yingbin Liang <br /> International Conference on Machine Learning (ICML) 2019 <b>(22.5% acceptance rate)</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2018/file/dea9ddb25cbf2352cf4dec30222a02a5-Paper.pdf">Minimax Estimation of Neural Net Distance</a> <br /> <b>Kaiyi Ji</b>, Yingbin Liang <br /> Conference on Neural Information Processing Systems (NeurIPS) 2018 <b>(21% acceptance rate)</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/8485860">Asymptotic Miss Ratio of LRU Caching with Consistent Hashing</a> <br /> <b>Kaiyi Ji</b>, Guocong Quan, Jian Tan <br /> IEEE INFOCOM 2018 <b>(19% acceptance rate)</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/8485891">LRU Caching with Dependent Competing Requests</a> <br /> Guocong Quan, <b>Kaiyi Ji</b>, Jian Tan <br /> IEEE INFOCOM 2018 <b>(19% acceptance rate)</b>.</p>
</li>
</ul>
<ul>
<li><p><a href="https://dl.acm.org/doi/10.1145/3179408">On Resource Pooling and Separation for LRU Caching</a> <br />   Jian Tan, Guocong Quan, <b>Kaiyi Ji</b>,  Ness Shroff <br /> ACM SIGMETRICS 2018 <b>(20% acceptance rate)</b>.</p>
</li>
</ul>
<h2>Journal Papers and Preprints</h2>
<ul>
<li><p><a href="https://arxiv.org/pdf/2010.07378.pdf">Boosting One-Point Derivative-Free Online Optimization via Residual Feedback</a> <br /> Yan Zhang, Yi Zhou, <b>Kaiyi Ji</b>, Michael M. Zavlanos <br /> To be submitted a journal. </p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2102.03926.pdf">Lower Bounds and Accelerated Algorithms for Bilevel Optimization</a> <br /> <b>Kaiyi Ji</b>, Yingbin Liang <br /> Submitted to Journal of Machine Learning Research (JMLR) 2021.</p>
</li>
</ul>
<ul>
<li><p>A New One-Point Residual-Feedback Oracle For Black-Box Learning and Control <br />  Yan Zhang, Yi Zhou, <b>Kaiyi Ji</b>, Michael M. Zavlanos <br /> Automatica 2021, to appear.</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/2002.07836.pdf">Theoretical Convergence of Multi-Step Model-Agnostic Meta-Learning</a> <br /> <b>Kaiyi Ji</b>, Junjie Yang, Yingbin Liang <br /> Journal of Machine Learning Research (JMLR) 2021 <br /> Short version appears at ICML 2021 Workshop: Beyond First-Order Methods in ML Systems</p>
</li>
</ul>
<ul>
<li><p><a href="https://ieeexplore.ieee.org/document/9330788">Understanding Estimation and Generalization Error of Generative Adversarial Networks</a> <br /> <b>Kaiyi Ji</b>, Yi Zhou, Yingbin Liang <br /> IEEE Trans. on Information Theory (TIT), 2021.</p>
</li>
</ul>
<ul>
<li><p><a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.354">When Will Gradient Methods Converge to Max-margin Classifier under ReLU Models?</a> <br /> Tengyu Xu, Yi Zhou, <b>Kaiyi Ji</b>, Yingbin Liang <br /> Stat, Special Issue of Deep Learning from Statistical Perspective, 2021</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/pdf/1802.05821.pdf">Learning Latent Features with Pairwise Penalties in Matrix Completion</a> <br />  <b>Kaiyi Ji</b>, Jian Tan, Yuejie Chi, Jinfeng Xu <br />  IEEE Trans. on Signal Processing (TSP), 2020 <br /> Short version as an invited paper to a special session at the IEEE SAM 2020</p>
</li>
</ul>
<ul>
<li><p>On Resource Pooling and Separation for LRU Caching <br />  Jian Tan, Guocong Quan, <b>Kaiyi Ji</b>,  Ness Shroff <br /> Proceedings of the ACM on Measurement and Analysis of Computing Systems (POMACS), 2018</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
